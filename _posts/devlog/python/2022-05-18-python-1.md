---
layout: post
title: requests와 BeautifulSoup를 이용한 웹사이트 크롤링
category: devlog
tags: python

last_modified_at: 2022-05-18T13:00:00+09:00
---

1. this list will be replaced by the toc
{:toc}

## requests란?
---
<img src="/assets/img/post-img/python/2022-05-18-python-1/requests.jpg" width=500>

+ 크롤링을 하려는 웹사이트는 HTML 문서로 이루어져 있다.
+ 이러한 HTML로 이루어진 웹사이트를 크롤링 하려면 HTML 문서를 가져오도록 request 해야 한다.
+ 그러기 위해서 파이썬에서 제공하는 requests 라이브러리를 이용해서 크롤링 하려는 웹사이트 HTML 문서를 가져오려고 한다.
    ~~~python
    requests.get('https://book.naver.com/bookdb/book_detail.naver?bid=1')
    ~~~

## BeautifulSoup란?
---
<img src="/assets/img/post-img/python/2022-05-18-python-1/bs.png" width=500>

+ requests를 이용하여 가져온 HTML 내용을 출력해보면 내용이 매우 복잡하고, 이 내용을 어떻게 써야할지 막막하다.
+ 이를 해결하기 위해 사용하는 BeautifulSoup는 크롤링 시 사용되는 파이썬의 라이브러리 중 하나이다.
+ requests를 이용하여 가져온 HTML 내용을 사용자 입맛에 맞게 가공하여 사용한다.
+ BeautifulSoup에 가져온 문서에 .content를 하고, "html.parser"를 실행하면, 웹사이트에서 F12키를 눌러 확인할 수 있는 HTML이 보여지게 된다.
    ~~~python
    soup = BeautifulSoup(response.content, "html.parser")
    ~~~



+ 이 후, 앞서 말했듯이 find, findAll과 같은 BeautifulSoup에서 제공하는 함수를 이용하여 사용자 입맛에 맞게 가공하면 된다.
    ~~~python
    h2_tag = soup.findAll('h2')
    ~~~

## 네이버북 크롤링
---
+ 나는 앞서 설명한 requests와 BeautifulSoup 라이브러리를 이용하여, 네이버북을 크롤링할 계획이다.
+ 예스24, 알라딘 등과 같은 도서 판매 웹사이트에서 작업을 안 한 이유는 네이버가 친숙하기도 하고, 해당 사이트들은 HTML을 까보니 데이터 정제하기가 까다로웠다.(한마디로 귀찮았다.)

## 마치며
---
+ 데이터 크롤링을 알아보고 시작한 이유는 현재 진행하고 있는 도서 대출 서비스 앱에서 도서 정보를 DB에 등록해놔야 하는데, 나는 이 작업을 내가 수작업으로 하기 귀찮았다...
+ 그래서 데이터 크롤링으로 도서 정보를 가져오고, 해당 도서를 DB에 추가하려고 제작하였다.
+ 또, 이러한 크롤링, DB에 정보 추가 작업을 내가 수작업으로(직접 파이썬을 실행시키고, 산출물을 DB에 추가하는 작업) 하기 싫어서, Github Action을 이용하여 CI / CD가 되도록 하였다.